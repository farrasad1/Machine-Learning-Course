{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "2O4pHnlYJRXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train data preprocessing"
      ],
      "metadata": {
        "id": "BZyiCztrG7_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/preprocess.py dataset/train-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO-KpHGtrqx5",
        "outputId": "2de4c002-8c6f-43ce-bb2a-3992af56f78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: dataset/train-processed-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test data preprocessing"
      ],
      "metadata": {
        "id": "HhGEcTCTHahr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/preprocess.py dataset/test-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsC3fvhTww9r",
        "outputId": "51b1f720-6f5c-4f4b-dce9-a69bb65d36c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"code/preprocess.py\", line 107, in <module>\n",
            "    preprocess_csv(csv_file_name, processed_file_name, test_file=False)\n",
            "  File \"code/preprocess.py\", line 81, in preprocess_csv\n",
            "    positive = int(line[:line.find(',')])\n",
            "ValueError: invalid literal for int() with base 10: 'is so sad for my apl friend'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show data train statistic have preprocessed"
      ],
      "metadata": {
        "id": "EGdjjFwPPhrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py dataset/train-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13q6uKkuw8HR",
        "outputId": "9f589646-eed7-45f7-d50a-ecdee0595397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/train-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/train-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 0, Avg: 0.0000, Max: 0\n",
            "URLs => Total: 0, Avg: 0.0000, Max: 0\n",
            "Emojis => Total: 0, Positive: 0, Negative: 0, Avg: 0.0000, Max: 0\n",
            "Words => Total: 1283269, Unique: 50361, Avg: 12.8327, Max: 41, Min: 0\n",
            "Bigrams => Total: 1183437, Unique: 392543, Avg: 11.8344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show data test statistic have preprocessed"
      ],
      "metadata": {
        "id": "4TqBv8jNP7yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/stats.py dataset/test-processed-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qy8QsZWxVCv",
        "outputId": "940a1502-e425-470f-fe50-a5f355d7f6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to dataset/test-processed-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to dataset/test-processed-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 0, Positive: 0, Negative: 0\n",
            "Traceback (most recent call last):\n",
            "  File \"code/stats.py\", line 106, in <module>\n",
            "    print 'User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions)\n",
            "ZeroDivisionError: float division by zero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shows the prediction accuracy of machine learning model"
      ],
      "metadata": {
        "id": "v29pu9nAQGJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the baseline accuracy"
      ],
      "metadata": {
        "id": "DB6bDd_JLgIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhR23YqPxaYJ",
        "outputId": "a767a227-f23b-46f0-854c-13b6db0b230f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the naive bayes accuracy"
      ],
      "metadata": {
        "id": "gqKv9ELnLnOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUStlIvCzCOO",
        "outputId": "66f9771e-5f98-4494-902b-ac48572de672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7787/10000 = 77.8700 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the maximum entropy"
      ],
      "metadata": {
        "id": "txwn3QbGLvIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/maxent-nltk.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2zBy9W0Cwo",
        "outputId": "87edb92f-e903-4959-bd12-7ded811b68ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.564\n",
            "         Final          -0.61649        0.756\n",
            "   1.000 ohhushfornewmoon==True and label is '1'\n",
            "   1.000 madnessness==True and label is '0'\n",
            "   1.000 alayellow==True and label is '1'\n",
            "   1.000 alltimelowweek==True and label is '1'\n",
            "   1.000 self_info==True and label is '1'\n",
            "   1.000 wompp==True and label is '0'\n",
            "   1.000 halfbloodprince==True and label is '1'\n",
            "   1.000 frenchmcflyteam==True and label is '1'\n",
            "   1.000 ouchmyshoulder==True and label is '0'\n",
            "   1.000 thaipbs==True and label is '1'\n",
            "Validation set accuracy:0.0000\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the decision tree accuracy"
      ],
      "metadata": {
        "id": "adgIj1YrL72F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seoawyMI7QzD",
        "outputId": "b11b7c60-0b9d-4c19-e94c-56219ff72deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6692/10000 = 66.9200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the random forest accuracy"
      ],
      "metadata": {
        "id": "aVjd8g0yL_dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgPgLPLx7swq",
        "outputId": "476bfbad-b720-4a8d-a56b-c9fb8bd251a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7189/10000 = 71.8900 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the XGBoost accuracy"
      ],
      "metadata": {
        "id": "9jMvHbRlMESc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = 'dataset/train-processed-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = 'dataset/train-processed-processed.csv'\n",
        "TEST_PROCESSED_FILE = 'dataset/test-processed-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 1500\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 100\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in range(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            utils.write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(20)\n",
        "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = utils.top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = utils.split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=20)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        utils.write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            utils.write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            utils.write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        utils.save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k5fMdznBvwR",
        "outputId": "9350f0ac-54b3-4f81-fc4d-261fb15551c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7161/10000 = 71.6100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the SVM accuracy"
      ],
      "metadata": {
        "id": "2yCFhQZCMLtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 code/svm.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81p7kIS-_nc1",
        "outputId": "6bd68171-1794-404d-c86c-8a1a21e60635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7850/10000 = 78.5000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Multi-Layer perceptron accuracy"
      ],
      "metadata": {
        "id": "WtxmADg1MOMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/neuralnet.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlNpn01aACa-",
        "outputId": "0d11f366-caca-4a29-c37e-0757ae68918f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "2022-01-22 07:07:30.255871: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Iteration 180/180, loss:0.5308, acc:0.7280\n",
            "Epoch: 1, val_acc:0.7583\n",
            "Accuracy improved from 0.0000 to 0.7583, saving model\n",
            "Iteration 180/180, loss:0.4549, acc:0.8180\n",
            "Epoch: 2, val_acc:0.7670\n",
            "Accuracy improved from 0.7583 to 0.7670, saving model\n",
            "Iteration 180/180, loss:0.4311, acc:0.7860\n",
            "Epoch: 3, val_acc:0.7617\n",
            "Iteration 180/180, loss:0.4644, acc:0.79002022-01-22 07:08:36.545674: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 4, val_acc:0.7665\n",
            "Iteration 180/180, loss:0.4190, acc:0.81602022-01-22 07:08:52.762815: W tensorflow/core/data/root_dataset.cc:163] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "\n",
            "Epoch: 5, val_acc:0.7627\n",
            "Testing\n",
            "Generating feature vectors\n",
            "\n",
            "\n",
            "Predicting batches\n",
            "\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download glove data"
      ],
      "metadata": {
        "id": "yaX3wsvmMa88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oae0s9qvBEsB",
        "outputId": "df4b08c6-9ac7-44e5-edf9-94d53f9f9cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-22 07:09:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-22 07:10:00--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-22 07:10:00--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.26MB/s    in 2m 40s  \n",
            "\n",
            "2022-01-22 07:12:40 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract glove data"
      ],
      "metadata": {
        "id": "HNygmvJBMg4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVEbNcn1CYeT",
        "outputId": "3ce9c055-3d8f-4b07-bed3-274ae2bf314b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the reccurent neural network accuracy"
      ],
      "metadata": {
        "id": "j5KHAgYuMjCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/lstm.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5_stEZtAfm8",
        "outputId": "11bb701f-f652-44bb-e13c-c09d23abfb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE vectors\n",
            "Processing 400000/0\n",
            "\n",
            "Found 31741 words in GLOVE\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-22 07:17:46.045124: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 200)           18000200  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 40, 200)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               168448    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,176,969\n",
            "Trainable params: 18,176,969\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "704/704 [==============================] - 15s 17ms/step - loss: 0.5439 - accuracy: 0.7217 - val_loss: 0.4875 - val_accuracy: 0.7642\n",
            "Epoch 2/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.4615 - accuracy: 0.7858 - val_loss: 0.4655 - val_accuracy: 0.7774\n",
            "Epoch 3/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.4125 - accuracy: 0.8150 - val_loss: 0.4863 - val_accuracy: 0.7754\n",
            "Epoch 4/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.3644 - accuracy: 0.8390 - val_loss: 0.4876 - val_accuracy: 0.7723\n",
            "Epoch 5/5\n",
            "704/704 [==============================] - 11s 16ms/step - loss: 0.3267 - accuracy: 0.8574 - val_loss: 0.5121 - val_accuracy: 0.7722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the convolutional neural network accuracy"
      ],
      "metadata": {
        "id": "0lzZwzX7MuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/cnn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13tVbhPFdWdV",
        "outputId": "a3748859-3da2-4ff2-dbdf-4ba29c02dfb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE seeds\n",
            "Processing 400000/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "2022-01-22 07:23:29.841058: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Epoch 1/8\n",
            "704/704 [==============================] - 27s 25ms/step - loss: 0.5331 - accuracy: 0.7243 - val_loss: 0.4700 - val_accuracy: 0.7755\n",
            "Epoch 2/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.4518 - accuracy: 0.7868 - val_loss: 0.4768 - val_accuracy: 0.7698\n",
            "Epoch 3/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.4002 - accuracy: 0.8176 - val_loss: 0.4560 - val_accuracy: 0.7811\n",
            "Epoch 4/8\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.3505 - accuracy: 0.8450 - val_loss: 0.4927 - val_accuracy: 0.7817\n",
            "Epoch 5/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.3098 - accuracy: 0.8652 - val_loss: 0.5092 - val_accuracy: 0.7759\n",
            "Epoch 6/8\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.2737 - accuracy: 0.8805 - val_loss: 0.5285 - val_accuracy: 0.7721\n",
            "Epoch 7/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.2465 - accuracy: 0.8936 - val_loss: 0.5928 - val_accuracy: 0.7563\n",
            "Epoch 8/8\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.2239 - accuracy: 0.9048 - val_loss: 0.6260 - val_accuracy: 0.7651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Majority Vote Ensemble"
      ],
      "metadata": {
        "id": "rxBcjgkcMyr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 code/extract-cnn-feats.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7lNESwxr6B3",
        "outputId": "f69a90cb-8efe-41b4-de49-36053ce1dda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for GLOVE seeds\n",
            "Processing 400000/0\n",
            "\n",
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"code/extract-cnn-feats.py\", line 85, in <module>\n",
            "    model = load_model(sys.argv[1])\n",
            "IndexError: list index out of range\n"
          ]
        }
      ]
    }
  ]
}